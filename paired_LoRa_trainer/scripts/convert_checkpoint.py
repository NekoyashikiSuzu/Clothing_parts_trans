#!/usr/bin/env python3
# scripts/convert_checkpoint.py - æ£€æŸ¥ç‚¹è½¬æ¢è„šæœ¬
import os
import sys
import argparse
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from models.flux_model import FluxLoRAModel
from utils.model_utils import get_model_size

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="FLUX LoRAæ£€æŸ¥ç‚¹è½¬æ¢å·¥å…·")
    
    parser.add_argument("--input_checkpoint", type=str, required=True, help="è¾“å…¥æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--base_model", type=str, default="black-forest-labs/FLUX.1-fill-dev", help="åŸºç¡€æ¨¡å‹")
    parser.add_argument("--format", type=str, default="safetensors", choices=["safetensors", "pytorch"], help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("--merge_weights", action="store_true", help="åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹")
    parser.add_argument("--float16", action="store_true", help="è½¬æ¢ä¸ºfloat16ç²¾åº¦")
    
    return parser.parse_args()

def load_checkpoint(checkpoint_path: str):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    print(f"åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    print(f"æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
    
    # æ˜¾ç¤ºæ£€æŸ¥ç‚¹ä¿¡æ¯
    if 'step' in checkpoint:
        print(f"  è®­ç»ƒæ­¥æ•°: {checkpoint['step']}")
    if 'epoch' in checkpoint:
        print(f"  è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
    if 'best_val_loss' in checkpoint:
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {checkpoint['best_val_loss']:.4f}")
    
    return checkpoint

def convert_to_lora_only(checkpoint, output_path: str, format_type: str = "safetensors"):
    """è½¬æ¢ä¸ºçº¯LoRAæƒé‡"""
    print(f"ğŸ”§ è½¬æ¢ä¸ºLoRAæƒé‡æ ¼å¼...")
    
    # æå–LoRAç›¸å…³çš„æƒé‡
    lora_state_dict = {}
    model_state_dict = checkpoint.get('model_state_dict', checkpoint)
    
    for key, value in model_state_dict.items():
        if 'lora_' in key or 'peft' in key:
            lora_state_dict[key] = value
    
    if not lora_state_dict:
        print("âš ï¸ æœªæ‰¾åˆ°LoRAæƒé‡ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–é€‚é…å™¨æƒé‡...")
        # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„é€‚é…å™¨æƒé‡
        for key, value in model_state_dict.items():
            if any(pattern in key.lower() for pattern in ['adapter', 'lora', 'delta']):
                lora_state_dict[key] = value
    
    if not lora_state_dict:
        raise ValueError("æ£€æŸ¥ç‚¹ä¸­æœªæ‰¾åˆ°LoRAæƒé‡")
    
    print(f"  æ‰¾åˆ° {len(lora_state_dict)} ä¸ªLoRAå‚æ•°")
    
    # ä¿å­˜LoRAæƒé‡
    if format_type == "safetensors":
        try:
            from safetensors.torch import save_file
            save_file(lora_state_dict, output_path)
        except ImportError:
            print("âš ï¸ safetensorsæœªå®‰è£…ï¼Œä½¿ç”¨PyTorchæ ¼å¼ä¿å­˜")
            torch.save(lora_state_dict, output_path.replace('.safetensors', '.pt'))
    else:
        torch.save(lora_state_dict, output_path)
    
    print(f"LoRAæƒé‡å·²ä¿å­˜: {output_path}")

def merge_lora_weights(checkpoint, base_model_name: str, output_dir: str):
    """åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹"""
    print(f"åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹...")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = FluxLoRAModel(
        model_name=base_model_name,
        device='cpu'
    )
    
    # åŠ è½½LoRAæƒé‡
    model.load_state_dict(checkpoint.get('model_state_dict', checkpoint), strict=False)
    
    # åˆå¹¶æƒé‡
    merged_model = model.merge_and_unload()
    
    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    merged_path = os.path.join(output_dir, "merged_model")
    merged_model.save_pretrained(merged_path)
    
    print(f"åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜: {merged_path}")
    
    return merged_path

def convert_precision(state_dict, target_dtype=torch.float16):
    """è½¬æ¢æ¨¡å‹ç²¾åº¦"""
    print(f"è½¬æ¢ç²¾åº¦åˆ°: {target_dtype}")
    
    converted_dict = {}
    for key, value in state_dict.items():
        if value.dtype.is_floating_point:
            converted_dict[key] = value.to(target_dtype)
        else:
            converted_dict[key] = value
    
    return converted_dict

def validate_converted_model(model_path: str, base_model_name: str):
    """éªŒè¯è½¬æ¢åçš„æ¨¡å‹"""
    print(f"éªŒè¯è½¬æ¢åçš„æ¨¡å‹...")
    
    try:
        # å°è¯•åŠ è½½æ¨¡å‹
        model = FluxLoRAModel(
            model_name=base_model_name,
            device='cpu'
        )
        
        if os.path.isdir(model_path):
            model.load_lora_weights(model_path)
        else:
            checkpoint = torch.load(model_path, map_location='cpu')
            model.load_state_dict(checkpoint, strict=False)
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = get_model_size(model)
        print(f"  æ¨¡å‹å‚æ•°æ•°: {model_info['total_parameters']:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {model_info['trainable_parameters']:,}")
        
        print("æ¨¡å‹éªŒè¯é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print("FLUX LoRAæ£€æŸ¥ç‚¹è½¬æ¢å·¥å…·")
    print("=" * 40)
    
    try:
        # åŠ è½½åŸå§‹æ£€æŸ¥ç‚¹
        checkpoint = load_checkpoint(args.input_checkpoint)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output_dir, exist_ok=True)
        
        # è½¬æ¢ç²¾åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.float16:
            print("è½¬æ¢ä¸ºfloat16ç²¾åº¦...")
            if 'model_state_dict' in checkpoint:
                checkpoint['model_state_dict'] = convert_precision(
                    checkpoint['model_state_dict'], 
                    torch.float16
                )
            else:
                checkpoint = convert_precision(checkpoint, torch.float16)
        
        if args.merge_weights:
            # åˆå¹¶æƒé‡åˆ°åŸºç¡€æ¨¡å‹
            merged_path = merge_lora_weights(
                checkpoint, 
                args.base_model, 
                args.output_dir
            )
            
            # éªŒè¯åˆå¹¶åçš„æ¨¡å‹
            validate_converted_model(merged_path, args.base_model)
            
        else:
            # è½¬æ¢ä¸ºçº¯LoRAæ ¼å¼
            if args.format == "safetensors":
                output_file = os.path.join(args.output_dir, "pytorch_lora_weights.safetensors")
            else:
                output_file = os.path.join(args.output_dir, "pytorch_lora_weights.pt")
            
            convert_to_lora_only(checkpoint, output_file, args.format)
            
            # éªŒè¯è½¬æ¢åçš„LoRAæƒé‡
            validate_converted_model(output_file, args.base_model)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'original_checkpoint': args.input_checkpoint,
            'base_model': args.base_model,
            'conversion_format': args.format,
            'merged_weights': args.merge_weights,
            'float16': args.float16
        }
        
        if 'config' in checkpoint:
            metadata['training_config'] = checkpoint['config']
        
        metadata_path = os.path.join(args.output_dir, "conversion_metadata.json")
        import json
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
        
        print("\næ£€æŸ¥ç‚¹è½¬æ¢å®Œæˆï¼")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        if args.merge_weights:
            print("\nä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹:")
            print(f"  from diffusers import FluxFillPipeline")
            print(f"  pipeline = FluxFillPipeline.from_pretrained('{os.path.join(args.output_dir, 'merged_model')}')")
        else:
            print("\nä½¿ç”¨LoRAæƒé‡:")
            print(f"  model.load_lora_weights('{args.output_dir}')")
        
    except Exception as e:
        print(f"è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()